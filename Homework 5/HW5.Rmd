---
title: "HW5"
output: pdf_document
---

1.
(a) Sampling a random dataset from the model. Samples of $\alpha$ are generated by sampling from $t_4(0, 1)$ and multiplying the sample by 2

```{r}
J <- 10
x <- runif(J)
alpha <- 2 * rt(1, 4)
beta <- rt(1, 4)
rtpois <- function(n, lambda)
  qpois(runif(n, dpois(0, lambda), 1), lambda)
n <- rtpois(J, 5)
inv_logit <- function(x)
  exp(x)/(1 + exp(x))
theta <- inv_logit(alpha + beta * x)
y <- rbinom(J, n, theta)
```

$\alpha$ is 
```{r, comment=NA}
alpha
```

and $\beta$ is
```{r, comment=NA}
beta
```

(b) Sampling from the posterior with rejection sampling using the prior for the g distribution. M is chosen adaptively by starting at 1/100000 and then increasing if the acceptance probabilities are too high (i.e. greater than 1).

```{r}
rejection_sampling <- function(samps, y, x, n)
{  
  s <- 1
  alphas <- numeric(length = 1000)
  betas <- numeric(length = 1000)
  m <- 100000
  save <- FALSE
  max_lik <- 0
  while(save != TRUE)
  {
    while(s <= samps)
    {
      alpha <- 2 * rt(1, 4)
      beta <- rt(1, 4)
      theta <- inv_logit(alpha + beta * x)
      lik <- prod(dbinom(y, n, theta))
      accept_prob <- m * lik
      if(lik > max_lik)
      {
        max_lik <- lik
      }
      if(runif(1) < accept_prob)
      {
        alphas[s] <- alpha
        betas[s] <- beta
        s <- s + 1
      }
    }
    if(m * max_lik > 1)
    {
      m <- 1/ max_lik
    }
    else
    {
      save <- TRUE
    }
  }
  list(alphas, betas)
}
samps <- rejection_sampling(1000, y, x, n)
alphas <- unlist(samps[1])
betas <- unlist(samps[2])
plot(alphas, betas)
```

(c) Computing the mode and hessian numerically using nlm. nlm finds the mode by minimizing negative log likelihood 

```{r}
log_post <- function(alpha, beta)
{
  theta <- inv_logit(alpha + beta * x)
  (sum(dbinom(y, n, theta, log = TRUE)) + dt(beta, 4, log = TRUE) + dt(alpha/2, 4, log = TRUE))
}

vect_post <- Vectorize(log_post)

nl_post <- function(params)
{
  alpha <- params[1]
  beta <- params[2]
  - log_post(alpha, beta)
}

post_mode <- nlm(nl_post, c(0, 0), hessian = TRUE)

library(ellipse)
cov <- solve(post_mode$hessian)
plot(ellipse(cov, centre = post_mode$estimate), type = 'l', xlab="alpha", ylab="beta",)
points(post_mode$estimate)
```

(d) Importance sampling according to given model
```{r}
library(mvtnorm)
imp_samps <- rmvt(1000, sigma = cov, df = 4, delta = post_mode$estimate)
g_theta <- dmvt(imp_samps, sigma = cov, df = 4, delta = post_mode$estimate)
q_theta <- vect_post(imp_samps[, 1], imp_samps[, 2])
imp_weights <- exp(q_theta - g_theta)
exp_alpha <- sum(imp_samps[, 1] * imp_weights)/sum(imp_weights)
exp_beta <- sum(imp_samps[, 2] * imp_weights)/sum(imp_weights)
```

$E(\alpha|y)$
```{r, comment=NA}
exp_alpha
```
$E(\beta|y)$
```{r, comment=NA}
exp_beta
```

2. General metropolis algorithm. Takes an X matrix of any size and a y vector with the same length as the number of rows in X. Transition function is independent normal with mean 0 and variance 1 around current beta vector.
```{r}
log_post <- function(beta, y, X)
{
  log_lik <- sum(dpois(y, exp(X %*% beta), log = TRUE))
  log_prior <- sum(dcauchy(beta, scale = 2.5))
  log_lik + log_prior
}
metropolis <- function(init_beta, n_samps, y, X)
{
  samps <- matrix(nrow = n_samps, ncol = length(init_beta))
  beta <- init_beta
  post <- log_post(beta, y, X) 
  for(i in 1:n_samps)
  {
    beta_star <- beta + rnorm(length(beta))
    post_star <- log_post(beta_star, y, X)
    r <- min(exp(post_star - post), 1)
    if(rbinom(1, 1, r))
    {
      beta <- beta_star
      post <- post_star
    }
    samps[i, ] <- beta
  }
  samps
}
```

(b)
Simulating fake data according to the model:
```{r}
x1 <- runif(50)
x2 <- runif(50)
x3 <- runif(50)
X <- as.matrix(cbind(x1, x2, x3))
true_beta <- rcauchy(3, scale = 2.5)
y <- rpois(50, exp(X %*% true_beta))
```

Getting 3 chains of 1000 metropolis samples
```{r}
init_beta <- rcauchy(3, scale = 2.5)
met_samps1 <- metropolis(init_beta, 1000, y, X)
init_beta <- rcauchy(3, scale = 2.5)
met_samps2 <- metropolis(init_beta, 1000, y, X)
init_beta <- rcauchy(3, scale = 2.5)
met_samps3 <- metropolis(init_beta, 1000, y, X)
```

We can now estimate the betas with the posterior mean:
```{r}
good_samples <- rbind(met_samps1[501:1000,], met_samps2[501:1000,], met_samps3[501:1000,])
colMeans(good_samples)
```


And plotting chains to see if they properly mixed
```{r}
yrange <- range(c(met_samps1[, 1], met_samps2[, 1], met_samps3[, 1]))
plot(met_samps1[, 1], type="l", ylim=yrange, ylab="samples")
lines(met_samps2[, 1], type="l", ylim=yrange)
lines(met_samps3[, 1], type="l", ylim=yrange)

yrange <- range(c(met_samps1[, 2], met_samps2[, 2], met_samps3[, 2]))
plot(met_samps1[, 2], type="l", ylim=yrange, ylab="samples")
lines(met_samps2[, 2], type="l", ylim=yrange)
lines(met_samps3[, 2], type="l", ylim=yrange)

yrange <- range(c(met_samps1[, 3], met_samps2[, 3], met_samps3[, 3]))
plot(met_samps1[, 3], type="l", ylim=yrange, ylab="samples")
lines(met_samps2[, 3], type="l", ylim=yrange)
lines(met_samps3[, 3], type="l", ylim=yrange)
```

```{r engine='cat',engine.opts=list(file = "metropolis.stan", lang = "stan")}
data {
  int n;
  int p;
  matrix[n, p] X;
  int y[n];
}
parameters {
  vector[p] beta;
}
model {
  y ~ poisson(exp(X * beta));
}
```

Stan model with estimates
```{r, warning=FALSE, message=FALSE, results='hide'}
library(rstan)
n = 50
p = 3
fit <- stan("metropolis.stan")
```

Estimates for beta parameters (sorry! I couldn't figure out how to print it better)
```{r}
print(fit, pars = c("beta[1]", "beta[2]", "beta[3]"))
```